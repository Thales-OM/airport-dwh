# include:
  # - ./airflow/docker-compose.airflow.yaml # Apache Airflow services

services:
  postgres_master:
    container_name: postgres_master
    image: postgres:14.5
    restart: always
    volumes:
      - data:/var/lib/postgresql/data
      # Replication to slave
      - data-replica:/var/lib/postgresql/data-replica
      - ./init-script/config:/etc/postgresql/config
      - ./init-script:/etc/postgresql/init-script
      - ./source_db/ddl_init.sql:/docker-entrypoint-initdb.d/ddl_init.sql
      # - ./init-script/init.sh:/docker-entrypoint-initdb.d/init.sh
      # Debezium connector
      - ./debezium/pg-setup.sql:/docker-entrypoint-initdb.d/pg-setup.sql
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    # entrypoint: ["/bin/bash", "-c", "chmod +x /etc/postgresql/init-script/init.sh && /etc/postgresql/init-script/init.sh"]
    # command: ["bin/bash chmod +x /etc/postgresql/init-script/init.sh && /etc/postgresql/init-script/init.sh"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 10

  postgres_replica:
    container_name: postgres_replica
    image: postgres:14.5
    restart: always
    volumes:
      - data-replica:/var/lib/postgresql/data
      - ./init-script/replica-config:/etc/postgresql/config
      - ./source_db/ddl_init.sql:/docker-entrypoint-initdb.d/ddl_init.sql
    ports:
      - "5433:5432"
    depends_on:
      postgres_master:
        condition: service_healthy  # Wait for postgres_master to be healthy
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

  dwh_dds:
    container_name: dwh_dds
    image: postgres:14.5
    restart: always
    volumes:
      - dwh_dds_data:/var/lib/postgresql/data
      # - ./init-script/replica-config:/etc/postgresql/config
      - ./dds/ddl_init.sql:/docker-entrypoint-initdb.d/ddl_init.sql
    ports:
      - "5434:5432"
    depends_on:
      - postgres_master
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 30s
      timeout: 10s
      retries: 10

  zookeeper:
    image: zookeeper:3.7.0  # Use a more recent version
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
    restart: always

  kafka:
    image: bitnami/kafka:latest
    ports:
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_SERVERS: zookeeper:2181
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # KAFKA_LOG_DIRS: /kafka/logs
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      - zookeeper
    restart: always
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions.sh", "--bootstrap-server", "localhost:9092"]
      interval: 60s
      timeout: 10s
      retries: 5

  debezium:
    image: debezium/connect:3.0.0.Final
    # build:
      # context: debezium-jdbc
    ports:
      - "8083:8083"
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=my_connect_configs
      - OFFSET_STORAGE_TOPIC=my_connect_offsets
      - STATUS_STORAGE_TOPIC=my_connect_statuses
      - DEBEZIUM_POSTGRES_USER=kafka_replica
      - DEBEZIUM_POSTGRES_PASSWORD=kafka_replica
      - DEBEZIUM_POSTGRES_DB=postgres
      - DEBEZIUM_POSTGRES_SERVICE=postgres_master
      - DEBEZIUM_POSTGRES_PORT=5432
    depends_on:
      - kafka
    restart: always
    volumes:
      - ./debezium/connector-setup.sh:/etc/kafka/connector-setup.sh
    # command: ["/etc/kafka/connector-setup.sh"]

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    ports:
      - "9000:9000"
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
    depends_on:
      - kafka
    restart: always
  
  dmp_service:
    build: ./dmp_service
    environment:
      - KAFKA_BROKERCONNECT=kafka:9092
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_HOST=dwh_dds
      - POSTGRES_PORT=5432
      - TARGET_DDS_SCHEMA=dwh_detailed
    depends_on:
      kafka:
        condition: service_healthy
      dwh_dds: 
        condition: service_healthy

  db_analytical:
    container_name: db_analytical
    image: postgres:14.5
    restart: always
    volumes:
      - db_analytical_data:/var/lib/postgresql/data
      - ./db_analytical/ddl_init.sql:/docker-entrypoint-initdb.d/ddl_init.sql
    ports:
      - "5435:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=postgres
  
  airflow:
    container_name: airflow
    image: apache/airflow:2.10.4
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
      AIRFLOW_CONN_DWH_DDS: '{"conn_type": "postgres", "conn-login": "postgres", "password": "postgres", "host": "dwh_dds", "port": 5432, "schema": "postgres"}'
      AIRFLOW_CONN_DB_ANALYTICAL: '{"conn_type": "postgres", "conn-login": "postgres", "password": "postgres", "host": "db_analytical", "port": 5432, "schema": "postgres"}'
      AUTH_ROLE_PUBLIC: 'Admin' # DEV ONLY !!!
      # ETL variables
      ETL_SOURCE_CONN_ID: 'dwh_dds'
      ETL_TARGET_CONN_ID: 'db_analytical'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/include:/opt/airflow/include
      # - ./logs:/opt/airflow/logs
      # - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    depends_on:
      - dwh_dds
      - db_analytical
    command: ["airflow", "standalone"]  # Start the Airflow webserver
    # command: >
    #   bash -c "
    #   airflow standalone && 
    #   sleep 20 &&
    #   airflow connections add dwh_dds --conn-type 'postgres' --conn-login 'postgres' --conn-password 'postgres' --conn-host 'dwh_dds' --conn-port '5432' --conn-schema 'postgres' && 
    #   airflow connections add db_analytical --conn-type 'postgres' --conn-login 'postgres' --conn-password 'postgres' --conn-host 'db_analytical' --conn-port '5432' --conn-schema 'postgres'"
  
  grafana:
    image: grafana/grafana-oss:11.3.0
    ports:
      - "3000:3000"  # Expose Grafana on port 3000
    volumes:
      - ./provisioning:/etc/grafana/provisioning  # Mount the entire provisioning directory
    environment:
      # (DEV ONLY!!!)
      - GF_SECURITY_ADMIN_PASSWORD=admin  # Set admin password for Grafana
      # Disable auth - make available for local user without auth
      # - GF_AUTH_ANONYMOUS_ENABLED=true 
      # - GF_AUTH_ANONYMOUS_ORG_NAME=Main Org.
      # - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
      # - GF_AUTH_DISABLE_LOGIN_FORM=true
      - GF_INSTALL_PLUGINS=marcusolsson-json-datasource
    restart: unless-stopped
    depends_on:
      - db_analytical

volumes:
  data:
  data-replica:
  dwh_dds_data:
  db_analytical_data:
